# ~/.gc3/gc3utils.conf
#
#  THIS IS JUST AN EXAMPLE CONFIGURATION FILE.  If you do not change
#  this file from its default, jobs will only run on your computer and
#  nowhere else.  In order to harness the full power of GC3Pie, edit
#  the file and define resources available into your environment,
#  using the supplied ones as guides.
#
#  Please see:
#    http://gc3pie.readthedocs.org/en/master/users/configuration.html
#  for details on this file format and contents.


[DEFAULT]
# The `DEFAULT` section is entirely optional; if present, its values can
# be used to interpolate values in other sections, using the `%(name)s` syntax.
# See documentation of the `SafeConfigParser` object at:
#   http://docs.python.org/library/configparser.html
debug = 0


# Auth sections: [auth/name]
#
# You can have as many `auth/name` sections as you want;
# this allows you to define different auths for different resources.
# Each `resource/***` section can reference one (and one only) auth
# section.
#
# The following section shows how to define an auth section for
# accessing computers via SSH.  The same section can be re-used on all
# computers where your username and/or identity file (public/private
# key) is the same:
# 
# [auth/ssh_alice]
# type = ssh
# username = your_ssh_user_name_on_computer_alice
#
# If you have a different account name on some resources, you can create
# another auth section with, e.g.
#
# [auth/ssh_bob]
# type = ssh
# username = your_ssh_user_name_on_computer_bob


# Resource sections: [resource/<resource_name>]
#
# You can have as many `resource/name` sections as you want; this
# allows you to define many different resources.
#
# Resources are distinguished by the value of the `type` key:
#
#  type           meaning
#  =============  ======================================================================
#  lsf            an LSF cluster, accessed via bsub/bjobs over SSH
#  pbs            a TORQUE/OpenPBS cluster, accessed via qsub/qstat over SSH
#  sge            a (Sun/Oracle/Univa) Grid Engine cluster, accessed via qsub/qstat over SSH
#  slurm          a SLURM cluster, accessed via sbatch/squeue over SSH
#  shellcmd       run commands through the shell, on the local computer or over SSH
#  ec2+shellcmd   run commands on VMs created on a cloud with EC2-compatible APIs
#  openstack+shellcmd  run commands on VMs created on a cloud with OpenStack APIs
#
# Some keys are commmon to all resource types:
#
#    * type: Resource type, see above
#    * auth: the name of a valid auth/*** section; only the authentication 
#      	     section name (after the `/`) must be specified.
#            Only needed if launching jobs on a remote computer or cluster
#            over SSH.
#    * max_cores: Total number of cores provided by the resource
#    * max_cores_per_job: Maximum number of CPU cores that a job can request; 
#            a resource will be dropped during the brokering process if a job 
#            requests more cores than this.
#    * max_memory_per_core: Max amount of memory that a job can request
#    * max_walltime: Maximum job running time
#
#    * prologue: Used only when ``type`` is ``pbs``, ``lsf``,
#       ``slurm`` or ``sge``. The content of the `prologue` script will
#       be *inserted* into the submission script and it's executed before
#       the real application.  It is intended to execute some shell
#       commands needed to setup the execution environment before running
#       the application (e.g. running a `module load ...` command). The
#       script **must** be a valid, plain `/bin/sh` script.
#
#    * epilogue: Used only when ``type`` is ``pbs``, ``lsf``,
#       ``slurm`` or ``sge``. The content of the `epilogue` script will
#       be *inserted* into the submission script and it's executed after
#       the real application. The script **must** be a valid, plain
#       `/bin/sh` script.
#
#    * <application_name>_prologue: Same as ``prologue``, but it is
#      used only when ``<application_name>``  matches the name of the
#      application. Valid application names are: `zods`, `gamess`,
#      `turbomole`, `codeml`, `rosetta`, `rosetta_docking`, `geotop`
#
#    * <application_name>_epilogue: Same as ``epilogue``, but it is
#      used only when ``<application_name>``  matches the name of the
#      application. Valid application names are: `zods`, `gamess`,
#      `turbomole`, `codeml`, `rosetta`, `rosetta_docking`, `geotop`
#
# The following two sections should be enough to run on the local
# machine only; adjust the `max_cores` parameter to set the number of
# jobs that you would like to spawn concurrently

[auth/noauth]
type=none

[resource/localhost]
# change the following to `enabled=no` to quickly disable
enabled=yes
type=shellcmd
transport=local
auth=noauth
# where the GNU `time` command is located; 
# the default is fine on almost any Linux distribution;
# you might need to change this for MacOSX
#time_cmd=/usr/bin/time
# max_cores sets a limit on the number of cuncurrently-running jobs
max_cores=2
max_cores_per_job=2
# adjust the following to match the features of your local computer
max_memory_per_core=2 GB
max_walltime=2 hours
architecture=x64_64
# When `override` is set to ``yes``, the shellcmd backend will try to
# discover the actual architecture, number of cores and the total
# memory of the machine and only use the values found on the
# configuration file as a fallback. Default for the `override` option
# is ``no``.
#override = no


##############################
# Computing cluster examples #
##############################
#
# The following examples show how to set up GC3Pie to submit jobs to a
# batch-queueing cluster.  GC3Pie could be running on the cluster
# frontend/submission host (in which case no "auth" is needed), or
# could connect to the submission host over SSH.
#
# 1) A Sun Grid Engine cluster, accessed by SSH'ing to the
# front-end node to control jobs via qsub/qstat:
#
# [resource/schroedinger]
# enabled = no
# type = sge
# auth = ssh
# transport = ssh
# frontend = idesl1.uzh.ch
# architecture = x86_64
# max_cores = 4096
# max_cores_per_job = 1024
# max_memory_per_core = 3 GB
# max_walltime = 72 hours
# # see http://gridengine.info/2005/11/04/immediate-accounting-data-flushing
# # if you need to lower this
# accounting_delay = 15
# # you can specify alternate paths to the SGE commands, e.g., to use
# # wrapper scripts or to specify addtional options; GC3Pie will *append*
# # the standard arguments to what you specify here
# #qsub = /usr/local/bin/qsub -q whatever.q
# #qstat = /usr/local/bin/qstat
# #qacct = /usr/local/sbin/qacct.sh
# #qdel = /usr/local/bin/qdel

# 2) An LSF cluster, accessed directly (i.e., GC3Pie must be
# running on the cluster front-end node):
#
# [resource/brutus]
# enabled = no
# type = lsf
# auth = noauth
# transport = local
# frontend = brutus.ethz.ch
# architecture = x86_64
# max_cores = 8192
# max_cores_per_job = 1024
# max_memory_per_core = 3 GB
# max_walltime = 72 hours
# # you can specify alternate paths to the LSF commands, e.g., to use
# # wrapper scripts or to specify addtional options; GC3Pie will *append*
# # the standard arguments to what you specify here
# #bsub = bsub -R lustre
# #bjobs = bjobs
# #bkill = /path/to/my/bkill.sh
# #lshosts = lshosts

# 3) A SLURM cluster, connecting to the front-end node via SSH.  This
# example also demoes the use of the "prologue" to load environment
# modules prior to executing the payload of a job.
#
# [resource/hydra]
# enabled = yes
# type = slurm
# frontend = login.s3it.uzh.ch
# transport = ssh
# auth = ssh_uzh
# max_walltime = 1 day
# max_cores = 96
# max_cores_per_job = 64
# max_memory_per_core = 1 TiB
# architecture = x86_64
# prologue_contents = module load cluster/largemem


##################
# Cloud examples #
##################

# Everything below this line provides examples for configuring GC3Pie
# to work with EC2- and OpenStack-based IaaS clouds.  The
# configuration is more complex than the examples above, since a cloud
# backend needs information on how to access the cloud management
# interface *and* the virtual machines.

# 1) Start VMs using an OpenStack cloud infrastructure.  Values in the
# example are actually the correct ones for the "Science Cloud" IaaS
# cloud at UZH.

# Since GC3Pie needs to authenticate to the cloud management system
# (via an HTTP-based network API) and then to the VMs that runs the
# jobs (via SSH), two separate authentication sections are needed.

# [auth/sciencecloud]
#
# This section is used to authenticate to the cloud management system.
# You should rather load the credentials in the environment using
# OpenStack's RC scripts, rather than hard-coding them here in cleartext.
#
#type = openstack

# If `os_username` is empty, then the content of the `OS_USERNAME`
# environment variable will be used.
# 
#os_username =

# If `os_password` is empty, then the content of the `OS_PASSWORD`
# environment variable will be used.
# 
#os_password =

# If `os_project_name` is empty, then the content of the
# `OS_TENANT_NAME` environment variable will be used.
#
#os_project_name =


# [auth/ssh_user_ubuntu]
#
# This section is used to connect via SSH to VMs that run the computational jobs.
#
#type = ssh

# Ubuntu cloud VMs use the `ubuntu` account by default.
#username = ubuntu


# The `resource` section specifies parameters to deal with the cloud
# management system.  GC3Pie will create a new VM for each submitted
# job, and a resource associated to the VM; for now the only option
# for VM-level "subresources" is `shellcmd`.  In other words, GC3Pie
# will create one or more VMs, and then connect to each one via SSH in
# order to directly execute jobs by starting them through the shell.

#[resource/sciencecloud]
#
# For now the only valid type to use with OpenStack-based clouds is
# `openstack+shellcmd`.
# 
#type = openstack+shellcmd

# The following values affect the resource broker and determine how
# many jobs can fit in a VM (i.e., when a new VM will be started).
# Their meaning is otherwise exactly the same as with all other
# resources; see, e.g., the `localhost` resource of type `shellcmd`.
#
#max_cores_per_job = 32
#max_memory_per_core = 4 GB
#max_walltime = 24 hours
#max_cores = 32
#architecture = x86_64

#  What authentication credential should be used to talk to the cloud
#  management interface? (OpenStack Nova API)
#
#auth = sciencecloud

# If `os_auth_url` is empty, then the `OS_AUTH_URL` environment variable
# is will be used. 
# 
#os_auth_url = http://cloud.s3it.uzh.ch:5000/v2.0

# OpenStack Region for starting VMs.
#
#os_region = nova

# Maximum number of VM running at the same time. If not configured
# here, the number of concurrently-running VMs will still be limited
# by OpenStack's quota.
# 
#vm_pool_max_size = 

# VM flavor for newly-created VMs. If `instance_type` is empty or not
# defined, the first flavor returned by OpenStack will be used
# instead. If an `Application()` instance has an `os_instance_type`
# attribute, it will be used instead of the default value configured
# here.
# 
#instance_type = 1cpu-4ram-hpc
#
# Override the default instance type for applications with a specific
# `application_name` attribute::
#
#<application_name>_instance_type

# Default image id to use (OpenStack's UUID).
#
# [The following UUID identifies the "Ubuntu Server 14.04.2
# (2015-08-05)" VM image on Science Cloud.]
#image_id = acebe825-e0d4-44ee-a1dc-a831561a2ea9
#
# Override the default image for applications with name `<application_name>`:
#
#<application_name>_image_id = 

# Operating System memory overhead for each VM.  This value will be
# used to compute the memory availability of VMs with a given flavor
# when matching it with the application memory requirements:
#
# flavor.memory - vm_os_overhead >= application.requested_memory
#
# This should guarantee that the actual available memory of a given VM will
# satisfy the application requirements.
# 
#vm_os_overhead = 512 MiB

# Auth stanza (in the current configuration file) to use for
# connecting to VMs.
# 
#vm_auth = ssh_user_ubuntu

# In order to access newly-spawned VMs, they need to be configured to
# allow passwordless authentication with the user name configured
# above.  In both OpenStack and EC2 clouds, this is done by defining
# an SSH keypair and authorizing it on the VM.  GC3Pie can use an
# existing keypair (specified by name), and even create it if it does
# not exist on the remote VM infrastructure (for this, a public key
# file is needed).
# 
# If the keypair `keypair_name` does not exists, a new one will be
# created and the public key in `public_key` will be imported. If the
# keypair exists, the fingerprint will be checked and an error is
# raised if it does not match `public_key`s fingerpring.
#
# keypair_name = gc3pie
# public_key = ~/.ssh/id_rsa.pub

# GC3Pie is able to configure security groups associated to the VM.
#
# If a security group with the name is not found, a new one will be
# created.  If it does exist but not all rules defined in
# `security_group_rules` are present, missing rules will be added to
# the security group.
#
# security_group_name=gc3pie_ssh
# security_group_rules=tcp:22:22:0.0.0.0/0,icmp:-1:-1:0.0.0.0/0
#
# `security_group_rules` is a comma separated list of security
# rules. Each rule is basically an entry in the firewall to allow some
# internet traffic.  A rule specification has the form:
#
#   protocol:from_port:to_port:ipnetwork
#
# where:
#
#   `protocol` is one of "icmp", "udp", "tcp"
#   from_port  is an integer (use -1 for icmp)
#   to_port    is an integer (use -1 for icmp)
#   ipnetwork  is a network specification in the form `IPaddr/netmask` (0.0.0.0/0 for any IP address)


# 2) Start VMs using an EC2-compatible cloud infrastructure

# Again, two separate [auth/...] sections are needed for cloud-based
# computing, since GC3Pie needs to authenticate to the cloud
# management system (via an HTTP-based network API) and then to the
# VMs that runs the jobs (via SSH).

#[auth/ec2]
#
# This section is used to authenticate to the cloud management API.
# You should rather load the credentials in the environment using
# AWS's RC scripts, rather than hard-coding them here in cleartext.
#
#type = ec2
#
# If `ec2_access_key` is empty, then the `EC2_ACCESS_KEY` environment
# variable will be used.
# 
#ec2_access_key =
#
# If `ec2_secret_key` is empty, then the `EC2_SECRET_KEY` environment
# variable is will be used.
# 
#ec2_secret_key =


#[resource/ec2]
#
# For now the only valid type to use with EC2-compatible clouds is
# `ec2+shellcmd`.
# 
#type = ec2+shellcmd
#enabled = yes

# The following values affect the resource broker and determine how
# many jobs can fit in a VM (i.e., when a new VM will be started).
# Their meaning is otherwise exactly the same as with all other
# resources; see, e.g., the `localhost` resource of type `shellcmd`.
#
#max_cores_per_job = 8
#max_memory_per_core = 2 GB
#max_walltime = 8 hours
#max_cores = 32
#architecture = x86_64

# Credentials to use when authenticating to the cloud management API.
#
#auth = ec2

# If `ec2_url` is empty, then the `EC2_URL` environment variable
# is will be used, if any. If EC2_URL is empty too, the default url
# for AWS will be used.
# 
#ec2_url = http://cloud.gc3.uzh.ch:8773/services/Cloud

# EC2 Region to use. Not needed on AWS.
#
#ec2_region = nova

# Maximum number of VM running at the same time
# 
#vm_pool_max_size = 

# What `[auth /...]`stanza in the current configuration file to use
# for connecting to VMs. (For the definition of `auth/ssh_user_ubuntu`
# look above the "OpenStack" example section.)
# 
#vm_auth = ssh_user_ubuntu

# See the "OpenStack" example for an explanation of the
# keypair-related options.
#
# keypair_name = gc3pie
# public_key = ~/.ssh/id_dsa.pub

# Default instance type to use for creating VMs. If `instance_type` is
# empty or not defined, `m1.small` will be used instead. If an
# `Application()` instance has an `ec2_instance_type` attribute, it
# will be used instead of the default value configured here.
# 
#instance_type = m1.small
#
# Override the default instance type for applications with a specific
# `application_name` attribute::
#
#<application_name>_instance_type

# Default image id to use (in EC2 format).
# 
#image_id = ami-00000035
#
# Override the default image for applications with name `<application_name>`:
#
#<application_name>_image_id = 

# Default image name to use. The value of `image_name` is ignored if
# `image_id` is present. Moreover, an error is raised if more than one
# image with the same image name is available on the cloud.
#
#image_name =

# `user_data` is a script passed to the virtual machine and executed
# right after the startup procedure. Default value is the empty script.
#
#user_data =
#
# Please note that if your script needs to span multiple lines they
# must be indented with respect to the initial `user_data=` line. For
# instance::
# 
#     user_data = #!/bin/bash
#         apt-get update
#         apt-get install -y octave
#     apt-get install -y r-core # <= this line will NOT be included in the `user_data` script!
#
# Override `user_data` to use for application with name `<application_name>`:
#
#<application_name>_user_data = 

# The EC2 backend also supports setting security groups, see the
# example in the "OpenStack" section.
